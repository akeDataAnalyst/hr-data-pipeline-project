{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "128cdca4-5fc0-479e-a238-77fd23e20e52",
   "metadata": {},
   "source": [
    "### HR Data Pipeline Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d3824b-4658-48a3-9646-1e6a316f587b",
   "metadata": {},
   "source": [
    "#### Objective: \n",
    "To design, build, and automate a data pipeline that collects HR data from multiple simulated sources, cleanses it, and loads it into a data warehouse (Google BigQuery) for analysis and reporting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d46af2-4ae7-4549-86f8-26eede079eb7",
   "metadata": {},
   "source": [
    "#### Import library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6715b073-c176-4629-91a4-f46f626b108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the IDs from the environment variables\n",
    "PROJECT_ID = os.getenv('GCP_PROJECT_ID')\n",
    "DATASET_ID = os.getenv('BIGQUERY_DATASET_ID')\n",
    "TABLE_ID = os.getenv('BIGQUERY_TABLE_ID')\n",
    "\n",
    "# Set the project ID for the BigQuery client\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df744d54-dd0e-4cf8-a1f0-fb044e80cc9d",
   "metadata": {},
   "source": [
    "#### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af282ac7-51ab-4766-8de2-1490a55d9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "    \"\"\"Reads data from CSV files and returns pandas DataFrames.\"\"\"\n",
    "    print(\"Extracting data from CSV files...\")\n",
    "    ats_df = pd.read_csv('ats_data.csv')\n",
    "    hris_df = pd.read_csv('hris_data.csv')\n",
    "    survey_df = pd.read_csv('survey_data.csv')\n",
    "    print(\"Data extraction complete.\")\n",
    "    return ats_df, hris_df, survey_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cddfb-4ce8-45ea-a2a5-b40345c4fc49",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ad2d3f-2d31-4442-befa-6c7dffad9d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(ats_df, hris_df, survey_df):\n",
    "    \"\"\"Cleans, transforms, and merges the datasets.\"\"\"\n",
    "    print(\"Transforming data...\")\n",
    "\n",
    "    # Data Cleaning and Type Conversion \n",
    "    # Convert date columns to datetime objects for accurate calculations\n",
    "    ats_df['application_date'] = pd.to_datetime(ats_df['application_date'])\n",
    "    ats_df['hired_date'] = pd.to_datetime(ats_df['hired_date'])\n",
    "    hris_df['start_date'] = pd.to_datetime(hris_df['start_date'])\n",
    "    survey_df['survey_date'] = pd.to_datetime(survey_df['survey_date'])\n",
    "\n",
    "    # Feature Engineering \n",
    "    # 1. Calculate time-to-hire from ATS data for successful hires\n",
    "    ats_hired_df = ats_df[ats_df['hiring_outcome'] == 'Hired'].copy()\n",
    "    ats_hired_df['time_to_hire_days'] = (ats_hired_df['hired_date'] - ats_hired_df['application_date']).dt.days\n",
    "\n",
    "    # 2. Calculate employee tenure from HRIS data\n",
    "    hris_df['tenure_days'] = (pd.to_datetime('today') - hris_df['start_date']).dt.days\n",
    "\n",
    "    # Data Merging \n",
    "    # Merge HRIS and Survey data on employee_id\n",
    "    unified_df = pd.merge(hris_df, survey_df, on='employee_id', how='left')\n",
    "    \n",
    "    # Now, join the HRIS and ATS data using the shared employee_id\n",
    "    unified_df = pd.merge(unified_df, ats_hired_df[['candidate_id', 'hiring_outcome', 'time_to_hire_days']].rename(columns={'candidate_id': 'employee_id'}), on='employee_id', how='left')\n",
    "\n",
    "    # Handle missing values after merge\n",
    "    unified_df['engagement_score'].fillna(-1, inplace=True)\n",
    "    unified_df['time_to_hire_days'].fillna(-1, inplace=True)\n",
    "    unified_df['hiring_outcome'].fillna('Not Applicable', inplace=True)\n",
    "    \n",
    "    print(\"Data transformation complete. Unified dataset shape:\", unified_df.shape)\n",
    "    return unified_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178ff15-baed-4bf4-a026-3f15bf0b0bac",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f30826b-fc28-4275-87eb-9b1e73fd78bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from CSV files...\n",
      "Data extraction complete.\n",
      "Transforming data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKE\\AppData\\Local\\Temp\\ipykernel_11960\\282909017.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  unified_df['engagement_score'].fillna(-1, inplace=True)\n",
      "C:\\Users\\AKE\\AppData\\Local\\Temp\\ipykernel_11960\\282909017.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  unified_df['time_to_hire_days'].fillna(-1, inplace=True)\n",
      "C:\\Users\\AKE\\AppData\\Local\\Temp\\ipykernel_11960\\282909017.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  unified_df['hiring_outcome'].fillna('Not Applicable', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transformation complete. Unified dataset shape: (250, 12)\n",
      "Loading data into BigQuery table people_analytics.hr_unified_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKE\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\auth\\_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "C:\\Users\\AKE\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into BigQuery.\n",
      "\n",
      "ETL Pipeline execution finished successfully!\n"
     ]
    }
   ],
   "source": [
    "def load_data(df, dataset_id, table_id):\n",
    "    \"\"\"Loads the DataFrame into a Google BigQuery table.\"\"\"\n",
    "    print(f\"Loading data into BigQuery table {dataset_id}.{table_id}...\")\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Configure the table schema to ensure correct data types in BigQuery\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\") # WRITE_TRUNCATE overwrites the table\n",
    "\n",
    "    # Load the DataFrame to BigQuery\n",
    "    job = client.load_table_from_dataframe(df, f\"{dataset_id}.{table_id}\", job_config=job_config)\n",
    "    job.result()  # Wait for the job to complete\n",
    "    print(\"Data loaded successfully into BigQuery.\")\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    ats_data, hris_data, survey_data = extract_data()\n",
    "    unified_data = transform_data(ats_data, hris_data, survey_data)\n",
    "\n",
    "    # Pass the variables to the load function\n",
    "    load_data(unified_data, DATASET_ID, TABLE_ID)\n",
    "    print(\"\\nETL Pipeline execution finished successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c631238-d88a-45bf-9c00-1ec9043e911c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
